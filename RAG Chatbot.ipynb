{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8cokJLgC90uJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install chromadb\n",
        "!pip install unstructured\n",
        "!pip install pdfminer.six\n",
        "!pip install pillow_heif\n",
        "!pip install unstructured_inference\n",
        "!apt-get install poppler-utils\n",
        "!pip install unstructured[local-inference]\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSBTVt-uJMNU"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "!ollama pull llama3.1\n",
        "!ollama serve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2utE7qM9-K-"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import Ollama\n",
        "from langchain.embeddings.ollama import OllamaEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import numpy as np\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import gradio as gr\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.ollama import OllamaEmbeddings\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVgXfcHn-Y4E"
      },
      "outputs": [],
      "source": [
        "# Initialize the language model\n",
        "llm = Ollama(model=\"llama3.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFafqO2L-7qw"
      },
      "outputs": [],
      "source": [
        "# # Example usage of the language model\n",
        "# response = llm.invoke(\"Why is the sky blue?\")\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rRvyh3P_0sn"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"Engineering Order.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzgDx_Y5_6Hd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(\"The document contains\",len(pages),\"pages\\n\")\n",
        "print(\"----------------------------------------\\n\")\n",
        "firstpage = pages[0]\n",
        "print(\"Page 1 of the document is as follows:\\n\")\n",
        "print(firstpage.page_content[0:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iaWNCtxADf3"
      },
      "outputs": [],
      "source": [
        "# Initialize a text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=750\n",
        ")\n",
        "splits = text_splitter.split_documents(pages)\n",
        "print(\"Number of Splits:\", len(splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0QRw3xfANa-"
      },
      "outputs": [],
      "source": [
        "embedding = OllamaEmbeddings(model=\"llama3.1\")\n",
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = 'chroma/'\n",
        "!rm -rf ./docs/chroma\n",
        "\n",
        "print(\"Vectorization Process Started...\")\n",
        "try:\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=embedding,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    print(\"Vectorization Process Completed!\")\n",
        "except Exception as e:\n",
        "    print(\"Error during vectorization:\", str(e))\n",
        "\n",
        "vectordb._collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the language model\n",
        "llm = Ollama(model=\"llama3.1\")"
      ],
      "metadata": {
        "id": "f_8dprgYv5PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "Q5vLdw7PR9k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "10K50QW9Sb5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ],
      "metadata": {
        "id": "VqXOOQjASb2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is this document about?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "# result[\"result\"]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "rACZGitrSbzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.memory import ConversationBufferMemory\n",
        "# memory = ConversationBufferMemory(\n",
        "#     memory_key=\"chat_history\",\n",
        "#     return_messages=True\n",
        "# )"
      ],
      "metadata": {
        "id": "gboQfRwL2X3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chains import ConversationalRetrievalChain\n",
        "# retriever=vectordb.as_retriever()\n",
        "# qa = ConversationalRetrievalChain.from_llm(\n",
        "#     llm,\n",
        "#     retriever=retriever,\n",
        "#     memory=memory\n",
        "# )"
      ],
      "metadata": {
        "id": "aKzDcleV2iYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def alternatingly_agree(question, history):\n",
        "#         return result\n",
        "\n",
        "# gr.ChatInterface(alternatingly_agree).launch()"
      ],
      "metadata": {
        "id": "YjGsn5LOXadN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}